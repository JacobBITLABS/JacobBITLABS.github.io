---
layout: post
date: 2025-05-15 10:10:00-0400
inline: true
related_posts: false
---

ðŸš€ Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?, has been accepted to the ACL 2025 Findings!
